{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mathtoolbox A library of mathematical tools (regression, interpolation, dimensionality reduction, clustering, etc.) written in C++11 and Eigen . Algorithms Scattered Data Interpolation and Function Approximation rbf-interpolation : Radial basis function (RBF) network gaussian-process-regression : Gaussian process regression (GPR) Dimensionality Reduction and Low-Dimensional Embedding classical-mds : Classical multi-dimensional scaling (MDS) Numerical Optimization backtracking-line-search : Backtracking line search bayesian-optimization : Bayesian optimization bfgs : BFGS method gradient-descent : Gradient descent method l-bfgs : Limited-memory BFGS method strong-wolfe-conditions-line-search : Strong Wolfe conditions line search Linear Algebra matrix-inversion : Matrix inversion techniques Utilities acquisition-functions : Acquisition functions constants : Constants kernel-functions : Kernel functions probability-distributions : Probability distributions Dependencies Main Library Eigen http://eigen.tuxfamily.org/ ( brew install eigen ) Python Bindings pybind11 https://github.com/pybind/pybind11 (included as gitsubmodule) Examples optimization-test-function https://github.com/yuki-koyama/optimization-test-functions (included as gitsubmodule) Build and Installation mathtoolbox uses CMake https://cmake.org/ for building source codes. This library can be built, for example, by git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive cd mathtoolbox mkdir build cd build cmake ../ make and optionally it can be installed to the system by make install When the CMake parameter MATHTOOLBOX_BUILD_EXAMPLES is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON make When the CMake parameter MATHTOOLBOX_PYTHON_BINDINGS is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_PYTHON_BINDINGS=ON make Prerequisites macOS: brew install eigen Ubuntu: sudo apt install libeigen3-dev Gallery Projects Using mathtoolbox SelPh https://github.com/yuki-koyama/selph (for classical-mds ) Sequential Line Search https://github.com/yuki-koyama/sequential-line-search (for acquisition-functions , kernel-functions , and probability-distributions ) Licensing The MIT License.","title":"Introduction"},{"location":"#mathtoolbox","text":"A library of mathematical tools (regression, interpolation, dimensionality reduction, clustering, etc.) written in C++11 and Eigen .","title":"mathtoolbox"},{"location":"#algorithms","text":"","title":"Algorithms"},{"location":"#scattered-data-interpolation-and-function-approximation","text":"rbf-interpolation : Radial basis function (RBF) network gaussian-process-regression : Gaussian process regression (GPR)","title":"Scattered Data Interpolation and Function Approximation"},{"location":"#dimensionality-reduction-and-low-dimensional-embedding","text":"classical-mds : Classical multi-dimensional scaling (MDS)","title":"Dimensionality Reduction and Low-Dimensional Embedding"},{"location":"#numerical-optimization","text":"backtracking-line-search : Backtracking line search bayesian-optimization : Bayesian optimization bfgs : BFGS method gradient-descent : Gradient descent method l-bfgs : Limited-memory BFGS method strong-wolfe-conditions-line-search : Strong Wolfe conditions line search","title":"Numerical Optimization"},{"location":"#linear-algebra","text":"matrix-inversion : Matrix inversion techniques","title":"Linear Algebra"},{"location":"#utilities","text":"acquisition-functions : Acquisition functions constants : Constants kernel-functions : Kernel functions probability-distributions : Probability distributions","title":"Utilities"},{"location":"#dependencies","text":"","title":"Dependencies"},{"location":"#main-library","text":"Eigen http://eigen.tuxfamily.org/ ( brew install eigen )","title":"Main Library"},{"location":"#python-bindings","text":"pybind11 https://github.com/pybind/pybind11 (included as gitsubmodule)","title":"Python Bindings"},{"location":"#examples","text":"optimization-test-function https://github.com/yuki-koyama/optimization-test-functions (included as gitsubmodule)","title":"Examples"},{"location":"#build-and-installation","text":"mathtoolbox uses CMake https://cmake.org/ for building source codes. This library can be built, for example, by git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive cd mathtoolbox mkdir build cd build cmake ../ make and optionally it can be installed to the system by make install When the CMake parameter MATHTOOLBOX_BUILD_EXAMPLES is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON make When the CMake parameter MATHTOOLBOX_PYTHON_BINDINGS is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_PYTHON_BINDINGS=ON make","title":"Build and Installation"},{"location":"#prerequisites","text":"macOS: brew install eigen Ubuntu: sudo apt install libeigen3-dev","title":"Prerequisites"},{"location":"#gallery","text":"","title":"Gallery"},{"location":"#projects-using-mathtoolbox","text":"SelPh https://github.com/yuki-koyama/selph (for classical-mds ) Sequential Line Search https://github.com/yuki-koyama/sequential-line-search (for acquisition-functions , kernel-functions , and probability-distributions )","title":"Projects Using mathtoolbox"},{"location":"#licensing","text":"The MIT License.","title":"Licensing"},{"location":"acquisition-functions/","text":"acquisition-functions Acquisition functions for Bayesian optimization Header #include <mathtoolbox/acquisition-functions.hpp> Overview The following acquisition functions (and their derivatives with respect to the data point) are supported: Expected improvement (EI) Useful Resources Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Proc. NIPS '12, pp.2951--2959. Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598","title":"acquisition-functions"},{"location":"acquisition-functions/#acquisition-functions","text":"Acquisition functions for Bayesian optimization","title":"acquisition-functions"},{"location":"acquisition-functions/#header","text":"#include <mathtoolbox/acquisition-functions.hpp>","title":"Header"},{"location":"acquisition-functions/#overview","text":"The following acquisition functions (and their derivatives with respect to the data point) are supported: Expected improvement (EI)","title":"Overview"},{"location":"acquisition-functions/#useful-resources","text":"Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Proc. NIPS '12, pp.2951--2959. Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598","title":"Useful Resources"},{"location":"backtracking-line-search/","text":"backtracking-line-search A line search method for finding a step size that satisfies the Armijo (i.e., sufficient decrease) condition based on a simple backtracking procedure. Header #include <mathtoolbox/backtracking-line-search.hpp> Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.1). Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"backtracking-line-search"},{"location":"backtracking-line-search/#backtracking-line-search","text":"A line search method for finding a step size that satisfies the Armijo (i.e., sufficient decrease) condition based on a simple backtracking procedure.","title":"backtracking-line-search"},{"location":"backtracking-line-search/#header","text":"#include <mathtoolbox/backtracking-line-search.hpp>","title":"Header"},{"location":"backtracking-line-search/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.1).","title":"Math and Algorithm"},{"location":"backtracking-line-search/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"bayesian-optimization/","text":"bayesian-optimization Bayesian optimization (BO) is a black-box global optimization algorithm. During the process, this algorithm determines the next sampling point based on Bayesian inference of the latent function. BO is likely to find reasonable solutions with fewer function evaluations than other optimization algorithms. Thus, it is suitable for problems with expensive-to-evaluate objective functions. Header #include <mathtoolbox/bayesian-optimization.hpp> Internal Dependencies acquisition-functions gradient-descent gaussian-process-regression Math and Algorithm Surrogate Function Representation This implementation assumes the Gaussian process prior. That is, it performs Gaussian process regression using the observed data. (TODO) Acquisition Functions Currently, it supports the expected improvement (EI) function only. (TODO) Examples Optimizing a One-Dimensional Function Performance The following plot shows a result of optimizing a simple objective function: \\max_{\\mathbf{x} \\in [-1, 1]^{5}} \\{ - \\| \\mathbf{x} \\|^{2} \\} 50 times with random initial solutions. As a baseline, it also shows a result by the same setting except for using random uniform sampling instead of BO. // Define the target problem const int num_dims = 5; const auto f = [](const Eigen::VectorXd& x) { return - x.squaredNorm(); }; const auto lower_bound = Eigen::VectorXd::Constant(num_dims, -1.0); const auto upper_bound = Eigen::VectorXd::Constant(num_dims, +1.0); // Instantiate an optimizer mathtoolbox::BayesianOptimizer optimizer{f, lower_bound, upper_bound}; // Perform optimization iteration for (int i = 0; i < 50; ++i) { optimizer.Step(); } // Retrieve the found solution const Eigen::VectorXd x_star = optimizer.GetCurrentSolution(); Useful Resources Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598 Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. 2016. Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc. IEEE 104, 1, pp.148--175 (2016). DOI: https://doi.org/10.1109/JPROC.2015.2494218 (TODO)","title":"bayesian-optimization"},{"location":"bayesian-optimization/#bayesian-optimization","text":"Bayesian optimization (BO) is a black-box global optimization algorithm. During the process, this algorithm determines the next sampling point based on Bayesian inference of the latent function. BO is likely to find reasonable solutions with fewer function evaluations than other optimization algorithms. Thus, it is suitable for problems with expensive-to-evaluate objective functions.","title":"bayesian-optimization"},{"location":"bayesian-optimization/#header","text":"#include <mathtoolbox/bayesian-optimization.hpp>","title":"Header"},{"location":"bayesian-optimization/#internal-dependencies","text":"acquisition-functions gradient-descent gaussian-process-regression","title":"Internal Dependencies"},{"location":"bayesian-optimization/#math-and-algorithm","text":"","title":"Math and Algorithm"},{"location":"bayesian-optimization/#surrogate-function-representation","text":"This implementation assumes the Gaussian process prior. That is, it performs Gaussian process regression using the observed data. (TODO)","title":"Surrogate Function Representation"},{"location":"bayesian-optimization/#acquisition-functions","text":"Currently, it supports the expected improvement (EI) function only. (TODO)","title":"Acquisition Functions"},{"location":"bayesian-optimization/#examples","text":"","title":"Examples"},{"location":"bayesian-optimization/#optimizing-a-one-dimensional-function","text":"","title":"Optimizing a One-Dimensional Function"},{"location":"bayesian-optimization/#performance","text":"The following plot shows a result of optimizing a simple objective function: \\max_{\\mathbf{x} \\in [-1, 1]^{5}} \\{ - \\| \\mathbf{x} \\|^{2} \\} 50 times with random initial solutions. As a baseline, it also shows a result by the same setting except for using random uniform sampling instead of BO. // Define the target problem const int num_dims = 5; const auto f = [](const Eigen::VectorXd& x) { return - x.squaredNorm(); }; const auto lower_bound = Eigen::VectorXd::Constant(num_dims, -1.0); const auto upper_bound = Eigen::VectorXd::Constant(num_dims, +1.0); // Instantiate an optimizer mathtoolbox::BayesianOptimizer optimizer{f, lower_bound, upper_bound}; // Perform optimization iteration for (int i = 0; i < 50; ++i) { optimizer.Step(); } // Retrieve the found solution const Eigen::VectorXd x_star = optimizer.GetCurrentSolution();","title":"Performance"},{"location":"bayesian-optimization/#useful-resources","text":"Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598 Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. 2016. Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc. IEEE 104, 1, pp.148--175 (2016). DOI: https://doi.org/10.1109/JPROC.2015.2494218 (TODO)","title":"Useful Resources"},{"location":"bfgs/","text":"bfgs The BFGS method (BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods. Header #include <mathtoolbox/bfgs.hpp> Internal Dependencies strong-wolfe-conditions-line-search Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 6). Inverse Hessian Initialization This implementation adopts the strategy described in Equation 6.20: \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^T \\mathbf{s}_k}{\\mathbf{y}_k^T \\mathbf{y}_k} \\mathbf{I}. See the book for details. Line Search This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size. Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"bfgs"},{"location":"bfgs/#bfgs","text":"The BFGS method (BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods.","title":"bfgs"},{"location":"bfgs/#header","text":"#include <mathtoolbox/bfgs.hpp>","title":"Header"},{"location":"bfgs/#internal-dependencies","text":"strong-wolfe-conditions-line-search","title":"Internal Dependencies"},{"location":"bfgs/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 6).","title":"Math and Algorithm"},{"location":"bfgs/#inverse-hessian-initialization","text":"This implementation adopts the strategy described in Equation 6.20: \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^T \\mathbf{s}_k}{\\mathbf{y}_k^T \\mathbf{y}_k} \\mathbf{I}. See the book for details.","title":"Inverse Hessian Initialization"},{"location":"bfgs/#line-search","text":"This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size.","title":"Line Search"},{"location":"bfgs/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"classical-mds/","text":"classical-mds Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot. Header #include <mathtoolbox/classical-mds.hpp> Math Overview Given a distance (or dissimilarity) matrix of n elements \\mathbf{D} \\in \\mathbb{R}^{n \\times n} and a target dimensionality m , this technique calculates a set of m -dimensional coordinates for them: \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}. If the elements are originally defined in an m' -dimensional space ( m < m' ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding). Algorithm First, calculate the kernel matrix: \\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n}, where \\mathbf{H} is called the centering matrix and defined as \\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n}, and \\mathbf{D}^{(2)} is the squared distance matrix. Then, apply eigenvalue decomposition to \\mathbf{K} : \\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T. Finally, pick up the m -largest eigenvalues \\mathbf{\\Lambda}_m and corresponding eigenvectors \\mathbf{V}_m , and calculate \\mathbf{X} by \\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}. Usage This technique can be calculated by the following function: Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd& D, unsigned dim); where dim is the target dimensionality for embedding. Useful Resources Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: https://doi.org/10.1145/1559755.1559760","title":"classical-mds"},{"location":"classical-mds/#classical-mds","text":"Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot.","title":"classical-mds"},{"location":"classical-mds/#header","text":"#include <mathtoolbox/classical-mds.hpp>","title":"Header"},{"location":"classical-mds/#math","text":"","title":"Math"},{"location":"classical-mds/#overview","text":"Given a distance (or dissimilarity) matrix of n elements \\mathbf{D} \\in \\mathbb{R}^{n \\times n} and a target dimensionality m , this technique calculates a set of m -dimensional coordinates for them: \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}. If the elements are originally defined in an m' -dimensional space ( m < m' ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding).","title":"Overview"},{"location":"classical-mds/#algorithm","text":"First, calculate the kernel matrix: \\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n}, where \\mathbf{H} is called the centering matrix and defined as \\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n}, and \\mathbf{D}^{(2)} is the squared distance matrix. Then, apply eigenvalue decomposition to \\mathbf{K} : \\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T. Finally, pick up the m -largest eigenvalues \\mathbf{\\Lambda}_m and corresponding eigenvectors \\mathbf{V}_m , and calculate \\mathbf{X} by \\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}.","title":"Algorithm"},{"location":"classical-mds/#usage","text":"This technique can be calculated by the following function: Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd& D, unsigned dim); where dim is the target dimensionality for embedding.","title":"Usage"},{"location":"classical-mds/#useful-resources","text":"Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: https://doi.org/10.1145/1559755.1559760","title":"Useful Resources"},{"location":"constants/","text":"constants Constants used for various mathematical contexts. Header #include <mathtoolbox/constants.hpp> Constants are defined in the namespace mathtoolbox::constants . Pi pi = 3.14159265358979323846264338327950288; Note that those who use C++20 or later should use <numbers> instead.","title":"constants"},{"location":"constants/#constants","text":"Constants used for various mathematical contexts.","title":"constants"},{"location":"constants/#header","text":"#include <mathtoolbox/constants.hpp> Constants are defined in the namespace mathtoolbox::constants .","title":"Header"},{"location":"constants/#pi","text":"pi = 3.14159265358979323846264338327950288; Note that those who use C++20 or later should use <numbers> instead.","title":"Pi"},{"location":"gaussian-process-regression/","text":"gaussian-process-regression Gaussian process regression (GPR) for scattered data interpolation and function approximation. Header #include <mathtoolbox/gaussian-process-regression.hpp> Overview Input The input consists of a set of N scattered data points: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1}^{N}, where \\mathbf{x}_i \\in \\mathbb{R}^D is the i -th data point location in a D -dimensional space and y_i \\in \\mathbb{R} is its associated value. This input data is also denoted as \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N} and \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}. Here, each observed values y_i is assumed to be a noisy version of the corresponding latent function value f_i = f(\\mathbf{x}_i) . More specifically, y_i = f_i + \\delta, where \\delta \\sim \\mathcal{N}(0, \\sigma_n^{2}). \\sigma_n^{2} (the noise variance) is considered as one of the hyperparamters of this model. Output Given the data and the Gaussian process assumption, GPR can calculate the most likely value f_{*} and its variance \\text{Var}(f_{*}) for an arbitrary location \\mathbf{x}_{*} . The variance roughly indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points). Note that, as the predicted value follows a Gaussian, its 95%-confidence interval can be obtained by [ f_{*} - 1.96 \\sqrt{\\text{Var}(f_{*})}, f_{*} + 1.96 \\sqrt{\\text{Var}(f_{*})} ] . Math Covariance Function The automatic relevance determination (ARD) Matern 5/2 kernel is the default choice: k(\\mathbf{x}_p, \\mathbf{x}_q ; \\sigma_f^{2}, \\boldsymbol{\\ell}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. Mean Function A constant-value function is used: m(\\mathbf{x}) = 0. Data Normalization Optionally, this implementation offers an automatic data normalization functionality. If this is enabled, it applies the following normalization: y_{i} \\leftarrow s \\cdot \\frac{y_{i} - \\mu}{\\sigma} \\:\\: \\text{for} \\:\\: i = 1, \\ldots, N, where s is an empirically selected scaling coefficient. This normalization is sometimes useful for the regression to be more robust for various datasets without drastically changing hyperparameters. Selecting Hyperparameters There are two options for setting hyperparameters: Set manually Determined by the maximum likelihood estimation Maximum Likelihood Estimation Let \\boldsymbol{\\theta} be a concatenation of hyperparameters; that is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\sigma_{n}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}. In this approach, these hyperparameters are determined by solving the following numerical optimization problem: \\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}). In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm). An initial solution for this maximization needs to be specified. Usage Instantiation and Data Specification A GPR object is instantiated with data specification in its constructor: GaussianProcessRegression(const Eigen::MatrixXd& X, const Eigen::VectorXd& y, const KernelType kernel_type = KernelType::ArdMatern52); Hyperparameter Selection Hyperparameters are set by either void SetHyperparameters(double sigma_squared_f, double sigma_squared_n, const Eigen::VectorXd& length_scales); or void PerformMaximumLikelihood(double sigma_squared_f_initial, double sigma_squared_n_initial, const Eigen::VectorXd& length_scales_initial); Prediction Once a GPR object is instantiated and its hyperparameters are set, it is ready for prediction. For an unknown location \\mathbf{x} , the GPR object predicts the most likely value f by the following method: double PredictMean(const Eigen::VectorXd& x) const; It also predicts the standard deviation \\sqrt{\\text{Var}(f)} by the following method: double PredictStdev(const Eigen::VectorXd& x) const; Useful Resources Mark Ebden. 2015. Gaussian Processes: A Quick Introduction. arXiv:1505.02965 . Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"gaussian-process-regression"},{"location":"gaussian-process-regression/#gaussian-process-regression","text":"Gaussian process regression (GPR) for scattered data interpolation and function approximation.","title":"gaussian-process-regression"},{"location":"gaussian-process-regression/#header","text":"#include <mathtoolbox/gaussian-process-regression.hpp>","title":"Header"},{"location":"gaussian-process-regression/#overview","text":"","title":"Overview"},{"location":"gaussian-process-regression/#input","text":"The input consists of a set of N scattered data points: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1}^{N}, where \\mathbf{x}_i \\in \\mathbb{R}^D is the i -th data point location in a D -dimensional space and y_i \\in \\mathbb{R} is its associated value. This input data is also denoted as \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N} and \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}. Here, each observed values y_i is assumed to be a noisy version of the corresponding latent function value f_i = f(\\mathbf{x}_i) . More specifically, y_i = f_i + \\delta, where \\delta \\sim \\mathcal{N}(0, \\sigma_n^{2}). \\sigma_n^{2} (the noise variance) is considered as one of the hyperparamters of this model.","title":"Input"},{"location":"gaussian-process-regression/#output","text":"Given the data and the Gaussian process assumption, GPR can calculate the most likely value f_{*} and its variance \\text{Var}(f_{*}) for an arbitrary location \\mathbf{x}_{*} . The variance roughly indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points). Note that, as the predicted value follows a Gaussian, its 95%-confidence interval can be obtained by [ f_{*} - 1.96 \\sqrt{\\text{Var}(f_{*})}, f_{*} + 1.96 \\sqrt{\\text{Var}(f_{*})} ] .","title":"Output"},{"location":"gaussian-process-regression/#math","text":"","title":"Math"},{"location":"gaussian-process-regression/#covariance-function","text":"The automatic relevance determination (ARD) Matern 5/2 kernel is the default choice: k(\\mathbf{x}_p, \\mathbf{x}_q ; \\sigma_f^{2}, \\boldsymbol{\\ell}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters.","title":"Covariance Function"},{"location":"gaussian-process-regression/#mean-function","text":"A constant-value function is used: m(\\mathbf{x}) = 0.","title":"Mean Function"},{"location":"gaussian-process-regression/#data-normalization","text":"Optionally, this implementation offers an automatic data normalization functionality. If this is enabled, it applies the following normalization: y_{i} \\leftarrow s \\cdot \\frac{y_{i} - \\mu}{\\sigma} \\:\\: \\text{for} \\:\\: i = 1, \\ldots, N, where s is an empirically selected scaling coefficient. This normalization is sometimes useful for the regression to be more robust for various datasets without drastically changing hyperparameters.","title":"Data Normalization"},{"location":"gaussian-process-regression/#selecting-hyperparameters","text":"There are two options for setting hyperparameters: Set manually Determined by the maximum likelihood estimation","title":"Selecting Hyperparameters"},{"location":"gaussian-process-regression/#maximum-likelihood-estimation","text":"Let \\boldsymbol{\\theta} be a concatenation of hyperparameters; that is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\sigma_{n}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}. In this approach, these hyperparameters are determined by solving the following numerical optimization problem: \\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}). In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm). An initial solution for this maximization needs to be specified.","title":"Maximum Likelihood Estimation"},{"location":"gaussian-process-regression/#usage","text":"","title":"Usage"},{"location":"gaussian-process-regression/#instantiation-and-data-specification","text":"A GPR object is instantiated with data specification in its constructor: GaussianProcessRegression(const Eigen::MatrixXd& X, const Eigen::VectorXd& y, const KernelType kernel_type = KernelType::ArdMatern52);","title":"Instantiation and Data Specification"},{"location":"gaussian-process-regression/#hyperparameter-selection","text":"Hyperparameters are set by either void SetHyperparameters(double sigma_squared_f, double sigma_squared_n, const Eigen::VectorXd& length_scales); or void PerformMaximumLikelihood(double sigma_squared_f_initial, double sigma_squared_n_initial, const Eigen::VectorXd& length_scales_initial);","title":"Hyperparameter Selection"},{"location":"gaussian-process-regression/#prediction","text":"Once a GPR object is instantiated and its hyperparameters are set, it is ready for prediction. For an unknown location \\mathbf{x} , the GPR object predicts the most likely value f by the following method: double PredictMean(const Eigen::VectorXd& x) const; It also predicts the standard deviation \\sqrt{\\text{Var}(f)} by the following method: double PredictStdev(const Eigen::VectorXd& x) const;","title":"Prediction"},{"location":"gaussian-process-regression/#useful-resources","text":"Mark Ebden. 2015. Gaussian Processes: A Quick Introduction. arXiv:1505.02965 . Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"Useful Resources"},{"location":"gradient-descent/","text":"gradient-descent Gradient descent is a gradient-based local optimization method. This is probably the simplest method in this category. Header #include <mathtoolbox/gradient-descent.hpp> Internal Dependencies backtracking-line-search Math and Algorithm Line Search This implementation uses backtracking-line-search to find an appropriate step size. The initial step size needs to be specified as default_alpha . Stopping Criteria (TODO) Bound Conditions This implementation supports simple lower/upper bound conditions. Useful Resources Gradient descent - Wikipedia. https://en.wikipedia.org/wiki/Gradient_descent .","title":"gradient-descent"},{"location":"gradient-descent/#gradient-descent","text":"Gradient descent is a gradient-based local optimization method. This is probably the simplest method in this category.","title":"gradient-descent"},{"location":"gradient-descent/#header","text":"#include <mathtoolbox/gradient-descent.hpp>","title":"Header"},{"location":"gradient-descent/#internal-dependencies","text":"backtracking-line-search","title":"Internal Dependencies"},{"location":"gradient-descent/#math-and-algorithm","text":"","title":"Math and Algorithm"},{"location":"gradient-descent/#line-search","text":"This implementation uses backtracking-line-search to find an appropriate step size. The initial step size needs to be specified as default_alpha .","title":"Line Search"},{"location":"gradient-descent/#stopping-criteria","text":"(TODO)","title":"Stopping Criteria"},{"location":"gradient-descent/#bound-conditions","text":"This implementation supports simple lower/upper bound conditions.","title":"Bound Conditions"},{"location":"gradient-descent/#useful-resources","text":"Gradient descent - Wikipedia. https://en.wikipedia.org/wiki/Gradient_descent .","title":"Useful Resources"},{"location":"kernel-functions/","text":"kernel-functions Kernel functions for various techniques. Header #include <mathtoolbox/kernel-functions.hpp> Overview Automatic Relevance Determination (ARD) Squared Exponential Kernel This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right), where \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}. Automatic Relevance Determination (ARD) Matern 5/2 Kernel This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}. Useful Resources Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"kernel-functions"},{"location":"kernel-functions/#kernel-functions","text":"Kernel functions for various techniques.","title":"kernel-functions"},{"location":"kernel-functions/#header","text":"#include <mathtoolbox/kernel-functions.hpp>","title":"Header"},{"location":"kernel-functions/#overview","text":"","title":"Overview"},{"location":"kernel-functions/#automatic-relevance-determination-ard-squared-exponential-kernel","text":"This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right), where \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}.","title":"Automatic Relevance Determination (ARD) Squared Exponential Kernel"},{"location":"kernel-functions/#automatic-relevance-determination-ard-matern-52-kernel","text":"This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}.","title":"Automatic Relevance Determination (ARD) Matern 5/2 Kernel"},{"location":"kernel-functions/#useful-resources","text":"Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"Useful Resources"},{"location":"l-bfgs/","text":"l-bfgs The Limited-memory BFGS method (L-BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods. Header #include <mathtoolbox/l-bfgs.hpp> Internal Dependencies strong-wolfe-conditions-line-search Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 7). Inverse Hessian Initialization This implementation adopts the strategy described in Equation 7.20: \\mathbf{H}_k^0 \\leftarrow \\frac{\\mathbf{y}_{k - 1}^T \\mathbf{s}_{k - 1}}{\\mathbf{y}_{k - 1}^T \\mathbf{y}_{k - 1}} \\mathbf{I}. See the book for details. Line Search This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size. Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"l-bfgs"},{"location":"l-bfgs/#l-bfgs","text":"The Limited-memory BFGS method (L-BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods.","title":"l-bfgs"},{"location":"l-bfgs/#header","text":"#include <mathtoolbox/l-bfgs.hpp>","title":"Header"},{"location":"l-bfgs/#internal-dependencies","text":"strong-wolfe-conditions-line-search","title":"Internal Dependencies"},{"location":"l-bfgs/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 7).","title":"Math and Algorithm"},{"location":"l-bfgs/#inverse-hessian-initialization","text":"This implementation adopts the strategy described in Equation 7.20: \\mathbf{H}_k^0 \\leftarrow \\frac{\\mathbf{y}_{k - 1}^T \\mathbf{s}_{k - 1}}{\\mathbf{y}_{k - 1}^T \\mathbf{y}_{k - 1}} \\mathbf{I}. See the book for details.","title":"Inverse Hessian Initialization"},{"location":"l-bfgs/#line-search","text":"This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size.","title":"Line Search"},{"location":"l-bfgs/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"matrix-inversion/","text":"matrix-inversion Matrix inversion techniques. Header #include <mathtoolbox/matrix-inversion.hpp> Block Matrix Inversion About Inverse of a block (partitioned) matrix \\mathbf{X} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}, where \\mathbf{A} and \\mathbf{D} are square matrices, can be calculated as \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} ( \\mathbf{I} + \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} ) & - \\mathbf{A}^{-1} \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\\\ - ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} & ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\end{bmatrix} This technique is useful particularly when \\mathbf{A} is relatively large (compared to \\mathbf{D} ) and \\mathbf{A}^{-1} is known. API This module provides the following function: Eigen::MatrixXd GetInverseUsingUpperLeftBlockInverse(const Eigen::MatrixXd& matrix, const Eigen::MatrixXd& upper_left_block_inverse); where upper_left_block_inverse corresponds to \\mathbf{A}^{-1} . Performance (Casual Comparison) When \\mathbf{X} was a random matrix, and the size of \\mathbf{X} was 3,000 and that of \\mathbf{A} was 2,999, a naive approach (i.e., the LU decomposition from Eigen) took 5847 milliseconds to obtain \\mathbf{X}^{-1} while the block inversion approach took only 121 milliseconds. Useful Resources Block matrix - Wikipedia. https://en.wikipedia.org/wiki/Block_matrix .","title":"matrix-inversion"},{"location":"matrix-inversion/#matrix-inversion","text":"Matrix inversion techniques.","title":"matrix-inversion"},{"location":"matrix-inversion/#header","text":"#include <mathtoolbox/matrix-inversion.hpp>","title":"Header"},{"location":"matrix-inversion/#block-matrix-inversion","text":"","title":"Block Matrix Inversion"},{"location":"matrix-inversion/#about","text":"Inverse of a block (partitioned) matrix \\mathbf{X} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}, where \\mathbf{A} and \\mathbf{D} are square matrices, can be calculated as \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} ( \\mathbf{I} + \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} ) & - \\mathbf{A}^{-1} \\mathbf{B} ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\\\ - ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\mathbf{C} \\mathbf{A}^{-1} & ( \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} )^{-1} \\end{bmatrix} This technique is useful particularly when \\mathbf{A} is relatively large (compared to \\mathbf{D} ) and \\mathbf{A}^{-1} is known.","title":"About"},{"location":"matrix-inversion/#api","text":"This module provides the following function: Eigen::MatrixXd GetInverseUsingUpperLeftBlockInverse(const Eigen::MatrixXd& matrix, const Eigen::MatrixXd& upper_left_block_inverse); where upper_left_block_inverse corresponds to \\mathbf{A}^{-1} .","title":"API"},{"location":"matrix-inversion/#performance-casual-comparison","text":"When \\mathbf{X} was a random matrix, and the size of \\mathbf{X} was 3,000 and that of \\mathbf{A} was 2,999, a naive approach (i.e., the LU decomposition from Eigen) took 5847 milliseconds to obtain \\mathbf{X}^{-1} while the block inversion approach took only 121 milliseconds.","title":"Performance (Casual Comparison)"},{"location":"matrix-inversion/#useful-resources","text":"Block matrix - Wikipedia. https://en.wikipedia.org/wiki/Block_matrix .","title":"Useful Resources"},{"location":"probability-distributions/","text":"probability-distributions Probability distributions for statistical estimation. Header #include <mathtoolbox/probability-distributions.hpp> Overview The following probability distributions and their first derivatives are supported: Standard normal distribution: \\mathcal{N}(x \\mid 0, 1) Normal distribution: \\mathcal{N}(x \\mid \\mu, \\sigma^{2}) Log-normal distribution: \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) In statistical estimation, taking logarithms of probabilities is often necessary. For this purpose, the following probability distributions and their derivatives are supported: Log of log-normal distribution: \\log \\{ \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) \\} Useful Resources Normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Normal_distribution . Log-normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Log-normal_distribution .","title":"probability-distributions"},{"location":"probability-distributions/#probability-distributions","text":"Probability distributions for statistical estimation.","title":"probability-distributions"},{"location":"probability-distributions/#header","text":"#include <mathtoolbox/probability-distributions.hpp>","title":"Header"},{"location":"probability-distributions/#overview","text":"The following probability distributions and their first derivatives are supported: Standard normal distribution: \\mathcal{N}(x \\mid 0, 1) Normal distribution: \\mathcal{N}(x \\mid \\mu, \\sigma^{2}) Log-normal distribution: \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) In statistical estimation, taking logarithms of probabilities is often necessary. For this purpose, the following probability distributions and their derivatives are supported: Log of log-normal distribution: \\log \\{ \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) \\}","title":"Overview"},{"location":"probability-distributions/#useful-resources","text":"Normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Normal_distribution . Log-normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Log-normal_distribution .","title":"Useful Resources"},{"location":"rbf-interpolation/","text":"rbf-interpolation Radial basis function (RBF) network for scattered data interpolation and function approximation. Header #include <mathtoolbox/rbf-interpolation.hpp> Math Overview Given input data: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n}, this technique calculates an interpolated value y for a specified point \\mathbf{x} by y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|), where \\phi(\\cdot) is a user-selected RBF and w_1, \\cdots, w_n are weights that are calculated in pre-computation. Pre-Computation The weight values need to be calculated in pre-computation. Let \\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T and \\mathbf{\\Phi} = \\begin{bmatrix} \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_{n, 1} & \\cdots & \\phi_{n, n} \\end{bmatrix}, where \\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|). The following linear system is solved for \\mathbf{w} : \\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}. LU decomposition can be used for solving this problem. Pre-Computation with Regularization The original formulation above is not robust (i.e., overfitting can occur) when the data points are dense and noisy. For such scenarios, it is possible to add a regularization term into pre-computation. That is, the following minimization problem is solved: \\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}. The derivative of this objective function with respect to \\mathbf{w} is \\begin{eqnarray*} && \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\ &=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\ &=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\ &=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}. \\end{eqnarray*} Thus, the solution of the above minimization problem can be obtained by solving the below linear system: (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}. Usage First, instantiate the class RbfInterpolation . Via the constructor, an RBF can be specified from the following options: Gaussian ThinPlateSpline InverseQuadratic Linear By default, ThinPlateSpline (i.e., \\phi(x) = x^2 \\log(x) ) is chosen. Then, set the target scattered data by the method: void SetData(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); where \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} represents the data points and \\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T represents their values. Next, calculate the weight values by the method: void ComputeWeights(bool use_regularization = false, double lambda = 0.001); When use_regularization is set true , the weights are calculated in the manner of scattered data approximation, rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice. Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method double GetValue(const Eigen::VectorXd& x) const; Useful Resources Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). ACM, New York, NY, USA, Article 27, 69 pages. DOI: https://doi.org/10.1145/2614028.2615425","title":"rbf-interpolation"},{"location":"rbf-interpolation/#rbf-interpolation","text":"Radial basis function (RBF) network for scattered data interpolation and function approximation.","title":"rbf-interpolation"},{"location":"rbf-interpolation/#header","text":"#include <mathtoolbox/rbf-interpolation.hpp>","title":"Header"},{"location":"rbf-interpolation/#math","text":"","title":"Math"},{"location":"rbf-interpolation/#overview","text":"Given input data: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n}, this technique calculates an interpolated value y for a specified point \\mathbf{x} by y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|), where \\phi(\\cdot) is a user-selected RBF and w_1, \\cdots, w_n are weights that are calculated in pre-computation.","title":"Overview"},{"location":"rbf-interpolation/#pre-computation","text":"The weight values need to be calculated in pre-computation. Let \\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T and \\mathbf{\\Phi} = \\begin{bmatrix} \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_{n, 1} & \\cdots & \\phi_{n, n} \\end{bmatrix}, where \\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|). The following linear system is solved for \\mathbf{w} : \\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}. LU decomposition can be used for solving this problem.","title":"Pre-Computation"},{"location":"rbf-interpolation/#pre-computation-with-regularization","text":"The original formulation above is not robust (i.e., overfitting can occur) when the data points are dense and noisy. For such scenarios, it is possible to add a regularization term into pre-computation. That is, the following minimization problem is solved: \\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}. The derivative of this objective function with respect to \\mathbf{w} is \\begin{eqnarray*} && \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\ &=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\ &=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\ &=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}. \\end{eqnarray*} Thus, the solution of the above minimization problem can be obtained by solving the below linear system: (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}.","title":"Pre-Computation with Regularization"},{"location":"rbf-interpolation/#usage","text":"First, instantiate the class RbfInterpolation . Via the constructor, an RBF can be specified from the following options: Gaussian ThinPlateSpline InverseQuadratic Linear By default, ThinPlateSpline (i.e., \\phi(x) = x^2 \\log(x) ) is chosen. Then, set the target scattered data by the method: void SetData(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); where \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} represents the data points and \\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T represents their values. Next, calculate the weight values by the method: void ComputeWeights(bool use_regularization = false, double lambda = 0.001); When use_regularization is set true , the weights are calculated in the manner of scattered data approximation, rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice. Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method double GetValue(const Eigen::VectorXd& x) const;","title":"Usage"},{"location":"rbf-interpolation/#useful-resources","text":"Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). ACM, New York, NY, USA, Article 27, 69 pages. DOI: https://doi.org/10.1145/2614028.2615425","title":"Useful Resources"},{"location":"strong-wolfe-conditions-line-search/","text":"strong-wolfe-conditions-line-search A line search method for finding a step size that satisfies the strong Wolfe conditions (i.e., the Armijo (i.e., sufficient decrease) condition and the curvature condition). Header #include <mathtoolbox/strong-wolfe-conditions-line-search.hpp> Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.5). Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"strong-wolfe-conditions-line-search"},{"location":"strong-wolfe-conditions-line-search/#strong-wolfe-conditions-line-search","text":"A line search method for finding a step size that satisfies the strong Wolfe conditions (i.e., the Armijo (i.e., sufficient decrease) condition and the curvature condition).","title":"strong-wolfe-conditions-line-search"},{"location":"strong-wolfe-conditions-line-search/#header","text":"#include <mathtoolbox/strong-wolfe-conditions-line-search.hpp>","title":"Header"},{"location":"strong-wolfe-conditions-line-search/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.5).","title":"Math and Algorithm"},{"location":"strong-wolfe-conditions-line-search/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"}]}